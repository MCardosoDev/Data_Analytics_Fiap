{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip istall findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
    "from pyspark.sql import functions as f\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 20:04:00 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/05 20:04:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/05 20:04:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               hello|\n",
      "+--------------------+\n",
      "|Sucesso total, es...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''SELECT 'Sucesso total, estamos online' as hello''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count   : 561\n",
      "df.col ct  : 6\n",
      "df.columns : ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../../Data/Fase 3/banklist.csv', sep=',', inferSchema=True, header=True)\n",
    "\n",
    "print('df.count   :', df.count())\n",
    "print('df.col ct  :', len(df.columns))\n",
    "print('df.columns :', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------------+\n",
      "|Bank Name|City         |Closing Date|\n",
      "+---------+-------------+------------+\n",
      "|Bank Name|Barboursville|Closing Date|\n",
      "|Bank Name|Ericson      |Closing Date|\n",
      "|Bank Name|Newark       |Closing Date|\n",
      "|Bank Name|Maumee       |Closing Date|\n",
      "+---------+-------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('banklist')\n",
    "\n",
    "df_check = spark.sql('''select 'Bank Name', City, 'Closing Date' from banklist''')\n",
    "df_check.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Basics Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "|summary|           Bank Name|   City|  ST|             CERT|Acquiring Institution|Closing Date|\n",
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "|  count|                 561|    561| 561|              561|                  561|         561|\n",
      "|   mean|                null|   null|null|31685.68449197861|                 null|        null|\n",
      "| stddev|                null|   null|null|16446.65659309965|                 null|        null|\n",
      "|    min|1st American Stat...|Acworth|  AL|               91|      1st United Bank|    1-Aug-08|\n",
      "|    max|               ebank|Wyoming|  WY|            58701|  Your Community Bank|    9-Sep-11|\n",
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+\n",
      "|summary|   City|  ST|\n",
      "+-------+-------+----+\n",
      "|  count|    561| 561|\n",
      "|   mean|   null|null|\n",
      "| stddev|   null|null|\n",
      "|    min|Acworth|  AL|\n",
      "|    max|Wyoming|  WY|\n",
      "+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('City', 'ST').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count, Columns and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas   : 561\n",
      "Total de colunas   : 6\n",
      "Colunas : ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n",
      "Tipo de dados : [('Bank Name', 'string'), ('City', 'string'), ('ST', 'string'), ('CERT', 'int'), ('Acquiring Institution', 'string'), ('Closing Date', 'string')]\n",
      "Shema : StructType(List(StructField(Bank Name,StringType,true),StructField(City,StringType,true),StructField(ST,StringType,true),StructField(CERT,IntegerType,true),StructField(Acquiring Institution,StringType,true),StructField(Closing Date,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print('Total de linhas   :', df.count())\n",
    "print('Total de colunas   :', len(df.columns))\n",
    "print('Colunas :', df.columns)\n",
    "print('Tipo de dados :', df.dtypes)\n",
    "print('Shema :', df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bank Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ST: string (nullable = true)\n",
      " |-- CERT: integer (nullable = true)\n",
      " |-- Acquiring Institution: string (nullable = true)\n",
      " |-- Closing Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas   : 561\n",
      "Colunas : ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "print('Total de linhas   :', df.count())\n",
    "print('Colunas :', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|           Bank Name|            City|\n",
      "+--------------------+----------------+\n",
      "| First Bank of Idaho|         Ketchum|\n",
      "|Amcore Bank, Nati...|        Rockford|\n",
      "|        Venture Bank|           Lacey|\n",
      "|First State Bank ...|           Altus|\n",
      "|Valley Capital Ba...|            Mesa|\n",
      "|Michigan Heritage...|Farmington Hills|\n",
      "|Columbia Savings ...|      Cincinnati|\n",
      "|       Fidelity Bank|        Dearborn|\n",
      "|The Park Avenue Bank|        Valdosta|\n",
      "|Western Commercia...|  Woodland Hills|\n",
      "|        Syringa Bank|           Boise|\n",
      "|Republic Federal ...|           Miami|\n",
      "|Westside Communit...|University Place|\n",
      "|   First United Bank|           Crete|\n",
      "|HarVest Bank of M...|    Gaithersburg|\n",
      "|            BankEast|       Knoxville|\n",
      "|    Polk County Bank|        Johnston|\n",
      "|Colorado Capital ...|     Castle Rock|\n",
      "|         Access Bank|        Champlin|\n",
      "|Pacific National ...|   San Francisco|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(*['Bank Name', 'City'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+---------------------+--------------------+\n",
      "| CERT|Closing Date|    City|Acquiring Institution|           Bank Name|\n",
      "+-----+------------+--------+---------------------+--------------------+\n",
      "|34396|   24-Apr-09| Ketchum|      U.S. Bank, N.A.| First Bank of Idaho|\n",
      "| 3735|   23-Apr-10|Rockford|          Harris N.A.|Amcore Bank, Nati...|\n",
      "+-----+------------+--------+---------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_1 = list(set(df.columns) - {'CRET', 'ST'})\n",
    "df2 = df.select(*col_1)\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "|           bank_name|            City|state| cert|     acq_institution|closing_date|\n",
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "| First Bank of Idaho|         Ketchum|   ID|34396|     U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|        Rockford|   IL| 3735|         Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|           Lacey|   WA|22868|First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|           Altus|   OK| 9873|        Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|            Mesa|   AZ|58399|Enterprise Bank &...|   11-Dec-09|\n",
      "|Michigan Heritage...|Farmington Hills|   MI|34369|      Level One Bank|   24-Apr-09|\n",
      "|Columbia Savings ...|      Cincinnati|   OH|32284|United Fidelity B...|   23-May-14|\n",
      "|       Fidelity Bank|        Dearborn|   MI|33883|The Huntington Na...|   30-Mar-12|\n",
      "|The Park Avenue Bank|        Valdosta|   GA|19797|  Bank of the Ozarks|   29-Apr-11|\n",
      "|Western Commercia...|  Woodland Hills|   CA|58087|First California ...|    5-Nov-10|\n",
      "|        Syringa Bank|           Boise|   ID|34296|        Sunwest Bank|   31-Jan-14|\n",
      "|Republic Federal ...|           Miami|   FL|22846|     1st United Bank|   11-Dec-09|\n",
      "|Westside Communit...|University Place|   WA|33997|        Sunwest Bank|   11-Jan-13|\n",
      "|   First United Bank|           Crete|   IL|20685|Old Plank Trail C...|   28-Sep-12|\n",
      "|HarVest Bank of M...|    Gaithersburg|   MD|57766|            Sonabank|   27-Apr-12|\n",
      "|            BankEast|       Knoxville|   TN|19869|     U.S. Bank, N.A.|   27-Jan-12|\n",
      "|    Polk County Bank|        Johnston|   IA|14194| Grinnell State Bank|   18-Nov-11|\n",
      "|Colorado Capital ...|     Castle Rock|   CO|34522|First-Citizens Ba...|    8-Jul-11|\n",
      "|         Access Bank|        Champlin|   MN|16476|           PrinsBank|    7-May-10|\n",
      "|Pacific National ...|   San Francisco|   CA|30006|      U.S. Bank N.A.|   30-Oct-09|\n",
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df \\\n",
    "    .withColumnRenamed('Bank Name', 'bank_name') \\\n",
    "    .withColumnRenamed('Acquiring Institution', 'acq_institution') \\\n",
    "    .withColumnRenamed('Closing Date', 'closing_date') \\\n",
    "    .withColumnRenamed('ST', 'state') \\\n",
    "    .withColumnRenamed('CERT', 'cert')\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|state|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('state', col('ST'))\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add constant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+-----+-------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|state|country|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+-------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|     US|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|     US|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('country', lit('US'))\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+---------------------+------------+\n",
      "|           Bank Name|    City| ST|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop('CERT')\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Multiple Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------------+------------+\n",
      "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop(*['ST', 'CERT'])\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------------+------------+\n",
      "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = reduce(DataFrame.drop, ['CERT', 'ST'], df)\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count:  561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2.count:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3.count:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=============================================>        (169 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df4.count:  73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Equal to values\n",
    "df2 = df.where(df['ST'] == 'NE')\n",
    "\n",
    "# Between values\n",
    "df3 = df.where(df['CERT'].between('1000', '2000'))\n",
    "\n",
    "# Is inside multiple values\n",
    "df4 = df.where(df['ST'].isin('NE', 'IL'))\n",
    "\n",
    "print('df.count: ', df.count())\n",
    "print('df2.count: ', df2.count())\n",
    "print('df3.count: ', df3.count())\n",
    "print('df4.count: ', df4.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data using Logical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "|         Bank Name|   City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "|Ericson State Bank|Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.where((df['ST'] == 'NE') & (df['City'] == 'Ericson'))\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Values in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Replace 7 in the above dataframe with 17 at all instances\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre replace\n",
    "df.show(2)\n",
    "\n",
    "# Post replace\n",
    "print('Replace 7 in the above dataframe with 17 at all instances')\n",
    "df.na.replace(7,17).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 21:47:33 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/05 21:47:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/05 21:47:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySpark Dataframe From RDD').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PySpark DataFrame from an Existing RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('C', 85, 76, 87, 91), ('B', 85, 76, 87, 91), ('A', 85, 78, 96, 92), ('A', 92, 76, 89, 96)], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = ['id_person', 'value_1', 'value_2', 'value_3', 'value_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "marks_df = spark.createDataFrame(rdd, schema=sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(marks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_person: string (nullable = true)\n",
      " |-- value_1: long (nullable = true)\n",
      " |-- value_2: long (nullable = true)\n",
      " |-- value_3: long (nullable = true)\n",
      " |-- value_4: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+-------+\n",
      "|id_person|value_1|value_2|value_3|value_4|\n",
      "+---------+-------+-------+-------+-------+\n",
      "|        C|     85|     76|     87|     91|\n",
      "|        B|     85|     76|     87|     91|\n",
      "|        A|     85|     78|     96|     92|\n",
      "|        A|     92|     76|     89|     96|\n",
      "+---------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Manipulation in PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 11:09:24 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/06 11:09:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/06 11:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pysparkdf').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../../Data/Fase 3/cereal.csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+\n",
      "|                name|mfr|   rating|\n",
      "+--------------------+---+---------+\n",
      "|           100% Bran|  N|68.402973|\n",
      "|   100% Natural Bran|  Q|33.983679|\n",
      "|            All-Bran|  K|59.425505|\n",
      "|All-Bran with Ext...|  K|93.704912|\n",
      "|      Almond Delight|  R|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|29.509541|\n",
      "|         Apple Jacks|  K|33.174094|\n",
      "|             Basic 4|  G|37.038562|\n",
      "|           Bran Chex|  R|49.120253|\n",
      "|         Bran Flakes|  P|53.313813|\n",
      "|        Cap'n'Crunch|  Q|18.042851|\n",
      "|            Cheerios|  G|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|19.823573|\n",
      "|            Clusters|  G|40.400208|\n",
      "|         Cocoa Puffs|  G|22.736446|\n",
      "|           Corn Chex|  R|41.445019|\n",
      "|         Corn Flakes|  K|45.863324|\n",
      "|           Corn Pops|  K|35.782791|\n",
      "|       Count Chocula|  G|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|40.448772|\n",
      "+--------------------+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'mfr', 'rating').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- Calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Calories', df['calories'].cast('Integer')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Calories|count|\n",
      "+--------+-----+\n",
      "|     140|    3|\n",
      "|     120|   10|\n",
      "|     100|   17|\n",
      "|     130|    2|\n",
      "|      50|    3|\n",
      "|      80|    1|\n",
      "|     160|    1|\n",
      "|      70|    2|\n",
      "|      90|    7|\n",
      "|     110|   29|\n",
      "|     150|    2|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('Calories').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|        Puffed Wheat|  Q|   C|      50|      2|  0|     0|  1.0| 10.0|     0|    50|       0|    3|   0.5| 1.0|63.005645|\n",
      "|         Puffed Rice|  Q|   C|      50|      1|  0|     0|  0.0| 13.0|     0|    15|       0|    3|   0.5| 1.0|60.756112|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|      Shredded Wheat|  N|   C|      80|      2|  0|     0|  3.0| 16.0|     0|    95|       0|    1|  0.83| 1.0|68.235885|\n",
      "|Strawberry Fruit ...|  N|   C|      90|      2|  0|    15|  3.0| 15.0|     5|    90|      25|    2|   1.0| 1.0|59.363993|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|Shredded Wheat 'n...|  N|   C|      90|      3|  0|     0|  4.0| 19.0|     0|   140|       0|    1|   1.0|0.67|74.472949|\n",
      "|Shredded Wheat sp...|  N|   C|      90|      3|  0|     0|  3.0| 20.0|     0|   120|       0|    1|   1.0|0.67|72.801787|\n",
      "|   Nutri-grain Wheat|  K|   C|      90|      3|  0|   170|  3.0| 18.0|     2|    90|      25|    3|   1.0| 1.0|59.642837|\n",
      "|      Raisin Squares|  K|   C|      90|      2|  0|     0|  2.0| 15.0|     6|   110|      25|    3|   1.0| 0.5|55.333142|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "|   Grape Nuts Flakes|  P|   C|     100|      3|  1|   140|  3.0| 15.0|     5|    85|      25|    3|   1.0|0.88|52.076897|\n",
      "| Frosted Mini-Wheats|  K|   C|     100|      3|  0|     0|  3.0| 14.0|     7|   100|      25|    2|   1.0| 0.8|58.345141|\n",
      "|                Life|  Q|   C|     100|      4|  2|   150|  2.0| 12.0|     6|    95|      25|    2|   1.0|0.67|45.328074|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('calories').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------------------------------------------------+\n",
      "|                name|vitamins|CASE WHEN (vitamins >= 25) THEN rich in vitamins END|\n",
      "+--------------------+--------+----------------------------------------------------+\n",
      "|           100% Bran|      25|                                    rich in vitamins|\n",
      "|   100% Natural Bran|       0|                                                null|\n",
      "|            All-Bran|      25|                                    rich in vitamins|\n",
      "|All-Bran with Ext...|      25|                                    rich in vitamins|\n",
      "|      Almond Delight|      25|                                    rich in vitamins|\n",
      "|Apple Cinnamon Ch...|      25|                                    rich in vitamins|\n",
      "|         Apple Jacks|      25|                                    rich in vitamins|\n",
      "|             Basic 4|      25|                                    rich in vitamins|\n",
      "|           Bran Chex|      25|                                    rich in vitamins|\n",
      "|         Bran Flakes|      25|                                    rich in vitamins|\n",
      "|        Cap'n'Crunch|      25|                                    rich in vitamins|\n",
      "|            Cheerios|      25|                                    rich in vitamins|\n",
      "|Cinnamon Toast Cr...|      25|                                    rich in vitamins|\n",
      "|            Clusters|      25|                                    rich in vitamins|\n",
      "|         Cocoa Puffs|      25|                                    rich in vitamins|\n",
      "|           Corn Chex|      25|                                    rich in vitamins|\n",
      "|         Corn Flakes|      25|                                    rich in vitamins|\n",
      "|           Corn Pops|      25|                                    rich in vitamins|\n",
      "|       Count Chocula|      25|                                    rich in vitamins|\n",
      "|  Cracklin' Oat Bran|      25|                                    rich in vitamins|\n",
      "+--------------------+--------+----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'vitamins', when(df.vitamins >= '25', 'rich in vitamins')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|             Crispix|  K|   C|     110|      2|  0|   220|  1.0| 21.0|     3|    30|      25|    3|   1.0| 1.0|46.895644|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "|         Froot Loops|  K|   C|     110|      2|  1|   125|  1.0| 11.0|    13|    30|      25|    2|   1.0| 1.0|32.207582|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.calories >= '100').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isnull() / isnotnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "|name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|rating|\n",
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 13:28:02 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/07 13:28:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/07 13:28:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL Consultas e Seleções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Status|\n",
      "+------+\n",
      "|    OK|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''select  'OK' as Status''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../../../Data/Fase 3/cereal.csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation Data with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('cereal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql('''select * from cereal''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql(''' select * from cereal where type = 'C' ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.where(df['type'] == 'C')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|      Golden Grahams|  G|   C|     110|      1|  1|   280|  0.0| 15.0|     9|    45|      25|    2|   1.0|0.75|23.804043|\n",
      "|  Honey Nut Cheerios|  G|   C|     110|      3|  1|   250|  1.5| 11.5|    10|    90|      25|    1|   1.0|0.75|31.072217|\n",
      "|                 Kix|  G|   C|     110|      2|  1|   260|  0.0| 21.0|     3|    40|      25|    2|   1.0| 1.5|39.241114|\n",
      "|        Lucky Charms|  G|   C|     110|      2|  1|   180|  0.0| 12.0|    12|    55|      25|    2|   1.0| 1.0|26.734515|\n",
      "|Multi-Grain Cheerios|  G|   C|     100|      2|  1|   220|  2.0| 15.0|     6|    90|      25|    1|   1.0| 1.0|40.105965|\n",
      "|Oatmeal Raisin Crisp|  G|   C|     130|      3|  2|   170|  1.5| 13.5|    10|   120|      25|    3|  1.25| 0.5|30.450843|\n",
      "|     Raisin Nut Bran|  G|   C|     100|      3|  2|   140|  2.5| 10.5|     8|   140|      25|    3|   1.0| 0.5|  39.7034|\n",
      "|   Total Corn Flakes|  G|   C|     110|      2|  1|   200|  0.0| 21.0|     3|    35|     100|    3|   1.0| 1.0|38.839746|\n",
      "|   Total Raisin Bran|  G|   C|     140|      3|  1|   190|  4.0| 15.0|    14|   230|     100|    3|   1.5| 1.0|28.592785|\n",
      "|   Total Whole Grain|  G|   C|     100|      3|  1|   200|  3.0| 16.0|     3|   110|     100|    3|   1.0| 1.0|46.658844|\n",
      "|             Triples|  G|   C|     110|      2|  1|   250|  0.0| 21.0|     3|    60|      25|    3|   1.0|0.75|39.106174|\n",
      "|                Trix|  G|   C|     110|      1|  1|   140|  0.0| 13.0|    12|    25|      25|    2|   1.0| 1.0|27.753301|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.where(df['mfr'] == 'G')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select distinct no SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|type|mfr|\n",
      "+----+---+\n",
      "|   C|  P|\n",
      "|   C|  Q|\n",
      "|   C|  N|\n",
      "|   H|  Q|\n",
      "|   C|  R|\n",
      "|   H|  N|\n",
      "|   C|  G|\n",
      "|   H|  A|\n",
      "|   C|  K|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql(''' SELECT DISTINCT type, mfr FROM cereal ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHERE no SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql(''' SELECT * FROM cereal WHERE mfr = 'N' AND calories >= 100 ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------------+\n",
      "|mfr|type|total|total_calories|\n",
      "+---+----+-----+--------------+\n",
      "|  A|   H|    1|           100|\n",
      "|  P|   C|    9|           980|\n",
      "|  K|   C|   23|          2500|\n",
      "|  G|   C|   22|          2450|\n",
      "|  Q|   C|    7|           660|\n",
      "|  R|   C|    8|           920|\n",
      "|  Q|   H|    1|           100|\n",
      "|  N|   H|    1|           100|\n",
      "|  N|   C|    5|           420|\n",
      "+---+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql('''  SELECT mfr,\n",
    "                            type,\n",
    "                            COUNT(*) AS total,\n",
    "                            SUM(calories) AS total_calories\n",
    "                        FROM cereal\n",
    "                        GROUP BY mfr,\n",
    "                            type ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE WHEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-----+--------------+\n",
      "|mfr|type|type_new|total|total_calories|\n",
      "+---+----+--------+-----+--------------+\n",
      "|  A|   H|       B|    1|           100|\n",
      "|  P|   C|       A|    9|           980|\n",
      "|  K|   C|       A|   23|          2500|\n",
      "|  G|   C|       A|   22|          2450|\n",
      "|  Q|   C|       A|    7|           660|\n",
      "|  R|   C|       A|    8|           920|\n",
      "|  Q|   H|       B|    1|           100|\n",
      "|  N|   H|       B|    1|           100|\n",
      "|  N|   C|       A|    5|           420|\n",
      "+---+----+--------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql('''  SELECT mfr,\n",
    "                            type,\n",
    "                            CASE\n",
    "                                WHEN type = 'C' THEN 'A'\n",
    "                                WHEN type = 'H' THEN 'B'\n",
    "                                ELSE 'C'\n",
    "                            END AS type_new,\n",
    "                            COUNT(*) AS total,\n",
    "                            SUM(calories) AS total_calories\n",
    "                        FROM cereal\n",
    "                        GROUP BY mfr,\n",
    "                            type ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas Avançadas em SQL usando PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+------------+------------+------------------+-------------------+----------+\n",
      "|mfr|type|sum_calories|min_calories|max_calories|      avg_calories|count_distinct_name|count_name|\n",
      "+---+----+------------+------------+------------+------------------+-------------------+----------+\n",
      "|  A|   H|         100|         100|         100|             100.0|                  1|         1|\n",
      "|  G|   C|        2450|         100|         140|111.36363636363636|                 22|        22|\n",
      "|  K|   C|        2500|          50|         160|108.69565217391305|                 23|        23|\n",
      "|  N|   C|         420|          70|          90|              84.0|                  5|         5|\n",
      "|  N|   H|         100|         100|         100|             100.0|                  1|         1|\n",
      "|  P|   C|         980|          90|         120|108.88888888888889|                  9|         9|\n",
      "|  Q|   C|         660|          50|         120| 94.28571428571429|                  7|         7|\n",
      "|  Q|   H|         100|         100|         100|             100.0|                  1|         1|\n",
      "|  R|   C|         920|          90|         150|             115.0|                  8|         8|\n",
      "+---+----+------------+------------+------------+------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql('''\n",
    "                    SELECT mfr,\n",
    "                            type,\n",
    "                            SUM(calories) AS sum_calories,\n",
    "                            MIN(calories) AS min_calories,\n",
    "                            MAX(calories) AS max_calories,\n",
    "                            AVG(calories) AS avg_calories,\n",
    "                            COUNT(DISTINCT name) AS count_distinct_name,\n",
    "                            COUNT(name) AS count_name\n",
    "                    FROM cereal\n",
    "                    GROUP BY\n",
    "                        mfr,\n",
    "                        type\n",
    "                    ORDER BY 1, 2\n",
    "                    ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+-------------------+----------+\n",
      "|mfr|type|sum_calories|min_calories|max_calories|avg_calories|sum_carbo|min_carbo|max_carbo|avg_carbo|sum_vitamins|min_vitamins|max_vitamins|avg_vitamins|count_distinct_name|count_name|\n",
      "+---+----+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+-------------------+----------+\n",
      "|  A|   H|         100|         100|         100|      100.00|     16.0|     16.0|     16.0|    16.00|          25|          25|          25|       25.00|                  1|         1|\n",
      "|  G|   C|        2450|         100|         140|      111.36|    324.0|     10.5|     21.0|    14.73|         775|          25|         100|       35.23|                 22|        22|\n",
      "|  K|   C|        2500|          50|         160|      108.70|    348.0|      7.0|     22.0|    15.13|         800|          25|         100|       34.78|                 23|        23|\n",
      "|  N|   C|         420|          70|          90|       84.00|     75.0|      5.0|     20.0|    15.00|          50|           0|          25|       10.00|                  5|         5|\n",
      "|  N|   H|         100|         100|         100|      100.00|     21.0|     21.0|     21.0|    21.00|           0|           0|           0|        0.00|                  1|         1|\n",
      "|  P|   C|         980|          90|         120|      108.89|    119.0|     11.0|     17.0|    13.22|         225|          25|          25|       25.00|                  9|         9|\n",
      "|  Q|   C|         660|          50|         120|       94.29|     81.0|      8.0|     14.0|    11.57|         100|           0|          25|       14.29|                  7|         7|\n",
      "|  Q|   H|         100|         100|         100|      100.00|     -1.0|     -1.0|     -1.0|    -1.00|           0|           0|           0|        0.00|                  1|         1|\n",
      "|  R|   C|         920|          90|         150|      115.00|    141.0|     14.0|     23.0|    17.63|         200|          25|          25|       25.00|                  8|         8|\n",
      "+---+----+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql('''\n",
    "                    SELECT mfr,\n",
    "                            type,\n",
    "                            SUM(calories) AS sum_calories,\n",
    "                            MIN(calories) AS min_calories,\n",
    "                            MAX(calories) AS max_calories,\n",
    "                            CAST(AVG(calories) AS DECIMAL(10, 2)) AS avg_calories,\n",
    "                   \n",
    "                            SUM(carbo) AS sum_carbo,\n",
    "                            MIN(carbo) AS min_carbo,\n",
    "                            MAX(carbo) AS max_carbo,\n",
    "                            CAST(AVG(carbo) AS DECIMAL(10, 2)) AS avg_carbo,\n",
    "                   \n",
    "                            SUM(vitamins) AS sum_vitamins,\n",
    "                            MIN(vitamins) AS min_vitamins,\n",
    "                            MAX(vitamins) AS max_vitamins,\n",
    "                            CAST(AVG(vitamins) AS DECIMAL(10, 2)) AS avg_vitamins,\n",
    "\n",
    "                            COUNT(DISTINCT name) AS count_distinct_name,\n",
    "                            COUNT(name) AS count_name\n",
    "                    FROM cereal\n",
    "                    GROUP BY\n",
    "                        mfr,\n",
    "                        type\n",
    "                    ORDER BY 1, 2\n",
    "                    ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+-------------------+----------+\n",
      "|mfr|type|type_fruit|sum_calories|min_calories|max_calories|avg_calories|sum_carbo|min_carbo|max_carbo|avg_carbo|sum_vitamins|min_vitamins|max_vitamins|avg_vitamins|count_distinct_name|count_name|\n",
      "+---+----+----------+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+-------------------+----------+\n",
      "|  A|   H|   Abacaxi|         100|         100|         100|      100.00|     16.0|     16.0|     16.0|    16.00|          25|          25|          25|       25.00|                  1|         1|\n",
      "|  G|   C|    Goiaba|        2450|         100|         140|      111.36|    324.0|     10.5|     21.0|    14.73|         775|          25|         100|       35.23|                 22|        22|\n",
      "|  K|   C|    Banana|        2500|          50|         160|      108.70|    348.0|      7.0|     22.0|    15.13|         800|          25|         100|       34.78|                 23|        23|\n",
      "|  N|   C|      Maça|         420|          70|          90|       84.00|     75.0|      5.0|     20.0|    15.00|          50|           0|          25|       10.00|                  5|         5|\n",
      "|  N|   H|      Maça|         100|         100|         100|      100.00|     21.0|     21.0|     21.0|    21.00|           0|           0|           0|        0.00|                  1|         1|\n",
      "|  P|   C|    Tomate|         980|          90|         120|      108.89|    119.0|     11.0|     17.0|    13.22|         225|          25|          25|       25.00|                  9|         9|\n",
      "|  Q|   C|      Pera|         660|          50|         120|       94.29|     81.0|      8.0|     14.0|    11.57|         100|           0|          25|       14.29|                  7|         7|\n",
      "|  Q|   H|      Pera|         100|         100|         100|      100.00|     -1.0|     -1.0|     -1.0|    -1.00|           0|           0|           0|        0.00|                  1|         1|\n",
      "|  R|   C|       Uva|         920|          90|         150|      115.00|    141.0|     14.0|     23.0|    17.63|         200|          25|          25|       25.00|                  8|         8|\n",
      "+---+----+----------+------------+------------+------------+------------+---------+---------+---------+---------+------------+------------+------------+------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereal = spark.sql('''\n",
    "                    SELECT mfr,\n",
    "                            type,\n",
    "                            CASE\n",
    "                                WHEN mfr = 'A' THEN 'Abacaxi'\n",
    "                                WHEN mfr = 'G' THEN 'Goiaba'\n",
    "                                WHEN mfr = 'K' THEN 'Banana'\n",
    "                                WHEN mfr = 'N' THEN 'Maça'\n",
    "                                WHEN mfr = 'P' THEN 'Tomate'\n",
    "                                WHEN mfr = 'Q' THEN 'Pera'\n",
    "                                WHEN mfr = 'R' THEN 'Uva'\n",
    "                                ELSE 'NA'\n",
    "                            END AS type_fruit,\n",
    "                            SUM(calories) AS sum_calories,\n",
    "                            MIN(calories) AS min_calories,\n",
    "                            MAX(calories) AS max_calories,\n",
    "                            CAST(AVG(calories) AS DECIMAL(10, 2)) AS avg_calories,\n",
    "                   \n",
    "                            SUM(carbo) AS sum_carbo,\n",
    "                            MIN(carbo) AS min_carbo,\n",
    "                            MAX(carbo) AS max_carbo,\n",
    "                            CAST(AVG(carbo) AS DECIMAL(10, 2)) AS avg_carbo,\n",
    "                   \n",
    "                            SUM(vitamins) AS sum_vitamins,\n",
    "                            MIN(vitamins) AS min_vitamins,\n",
    "                            MAX(vitamins) AS max_vitamins,\n",
    "                            CAST(AVG(vitamins) AS DECIMAL(10, 2)) AS avg_vitamins,\n",
    "\n",
    "                            COUNT(DISTINCT name) AS count_distinct_name,\n",
    "                            COUNT(name) AS count_name\n",
    "                    FROM cereal\n",
    "                    GROUP BY\n",
    "                        mfr,\n",
    "                        type\n",
    "                    ORDER BY 1, 2\n",
    "                    ''')\n",
    "cereal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.csv('../../../Data/Fase 3/sales_data_samples.csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = spark.sql(''' \n",
    "                    SELECT DISTINCT orderdate,\n",
    "                        qtr_id,\n",
    "                        mount_id,\n",
    "                        year_id\n",
    "                    FROM sales\n",
    "                    ORDER BY orderdate\n",
    "                ''')\n",
    "\n",
    "sales_data = spark.sql(''' \n",
    "                    SELECT DISTINCT ordernumber,\n",
    "                        customername,\n",
    "                        orderdate,\n",
    "                        sales,\n",
    "                        quantityordered,\n",
    "                        productcode,\n",
    "                        orderlinenumber,\n",
    "                        priceeach\n",
    "                    FROM sales\n",
    "                    ORDER BY ordernumber\n",
    "                ''')\n",
    "\n",
    "customers = spark.sql(''' \n",
    "                    SELECT DISTINCT customername,\n",
    "                        phone,\n",
    "                        addressline1,\n",
    "                        addressline2,\n",
    "                        city,\n",
    "                        state,\n",
    "                        postalcode,\n",
    "                        territory\n",
    "                    FROM sales\n",
    "                    ORDER BY customername\n",
    "                ''')\n",
    "\n",
    "calendar.createOrReplaceTempView('calendar')\n",
    "sales_data.createOrReplaceTempView('sales_data')\n",
    "customers.createOrReplaceTempView('customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = spark.sql(''' \n",
    "                    SELECT *\n",
    "                    FROM sales_data s\n",
    "                    INNER JOIN customers c ON s.customername = c.customername\n",
    "                ''')\n",
    "master.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 14:52:41 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/07 14:52:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/07 14:52:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('PySpark DataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "myRDD = sc.parallelize(data)\n",
    "newRDD = myRDD.map(lambda x: x*2)\n",
    "print(newRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "myRDD = sc.parallelize(data)\n",
    "newRDD = myRDD.filter(lambda x: x%2 == 0)\n",
    "print(newRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "myRDD = sc.parallelize(data)\n",
    "newRDD = myRDD.distinct()\n",
    "print(newRDD.collect())#count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2, 3]), ('b', [1])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([('a', 1), ('a', 2), ('a', 3), ('b', 1)])\n",
    "resultList = myRDD.groupByKey().mapValues(list)\n",
    "resultList.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 6), ('b', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "myRDD = sc.parallelize([('a', 1), ('a', 2), ('a', 3), ('b', 1)])\n",
    "newRDD = myRDD.reduceByKey(add)\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 4), ('c', 1), ('d', 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([('c', 1), ('d', 2), ('a', 3), ('b', 4)])\n",
    "newRDD = myRDD.sortByKey() # type: ignore\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNION()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD1 = sc.parallelize([1, 2, 3, 4])\n",
    "myRDD2 = sc.parallelize([3, 4, 5, 6, 7])\n",
    "newRDD = myRDD1.union(myRDD2)\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 8]\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.reduce(lambda x, y: x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R\n",
      "Scala\n",
      "Python\n",
      "Java\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    print(x)\n",
    "\n",
    "data = ['Scala', 'Python', 'Java', 'R']\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.foreach(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Scala', 2), ('Python', 2), ('Java', 2), ('R', 2)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Scala', 'Python', 'Java', 'R', 'Scala', 'Python', 'Java', 'R']\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.countByValue().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('S', 2), ('P', 2), ('J', 2), ('R', 2)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Scala', 'Python', 'Java', 'R', 'Scala', 'Python', 'Java', 'R']\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.countByKey().items() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [2, 5, 3, 8, 4]\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [2, 5, 3, 8, 4]\n",
    "myRDD = sc.parallelize(data)\n",
    "myRDD.top(3) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0  3  4  6]\n",
      " [ 5  6  1  8  9]\n",
      " [ 8  0  0  5 10]]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[8, 0 , 3, 4, 6], [5, 6, 1, 8, 9], [8, 0, 0, 5, 10]])\n",
    "novoUser = [8, 0, 2, 3, 0]\n",
    "nao_assistidos = [0, 1, 0, 0, 1]\n",
    "print(matrix)\n",
    "nomeFilmes = ['Round 6', 'A Invocação do Mal', '9 Desconhecidos', 'You', 'La Casa de Papel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "5\n",
      "[5 6 1 8 9]\n",
      "0 [8 0 3 4 6]\n",
      "user: 0\n",
      "[8, 3, 4] [8, 2, 3]\n",
      "1 [5 6 1 8 9]\n",
      "user: 1\n",
      "[5, 1, 8] [8, 2, 3]\n",
      "2 [ 8  0  0  5 10]\n",
      "user: 2\n",
      "[8, 0, 5] [8, 2, 3]\n",
      "\n",
      "[0.9994237971287664, 0.23621543814299703, 0.8723686098443353]\n"
     ]
    }
   ],
   "source": [
    "# criar um vetor com 3 posições e preencher com zero\n",
    "similarity = [0] * 3\n",
    "print(matrix[0][0])\n",
    "print(matrix[1][0])\n",
    "print(matrix[1, :])\n",
    "# para cada usuário no sistema\n",
    "for i in range(0, 3):\n",
    "    # vamos pegar os dados desse usuário\n",
    "    temp = matrix[i, :]\n",
    "    print(i, temp)\n",
    "    # mas queremos apenas comparar os dados que o novo user assistiu (novoUser != 0)\n",
    "    tempUser = [t for n, t in zip(novoUser, temp) if n != 0 ]\n",
    "    tempNovoUser = [n for n in novoUser if n != 0]\n",
    "    # para verificar o processo\n",
    "    print('user:', i)\n",
    "    print(tempUser, tempNovoUser)\n",
    "    # calcular o pearson\n",
    "    similarity[i] = stats.pearsonr(tempUser, tempNovoUser)[0] # a função retorna dois valores e o primeiro\n",
    "print()\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nota_peso[ 0 ][ 0 ] = nao_assistidos[ 0 ] * matrix[ 0 ][ 0 ] * similarity[ 0 ]\n",
      "nota_peso[ 0 ][ 0 ] =  0  *  8  *  0.9994237971287664\n",
      "nota_peso[ 0 ][ 1 ] = nao_assistidos[ 1 ] * matrix[ 0 ][ 1 ] * similarity[ 0 ]\n",
      "nota_peso[ 0 ][ 1 ] =  1  *  0  *  0.9994237971287664\n",
      "nota_peso[ 0 ][ 2 ] = nao_assistidos[ 2 ] * matrix[ 0 ][ 2 ] * similarity[ 0 ]\n",
      "nota_peso[ 0 ][ 2 ] =  0  *  3  *  0.9994237971287664\n",
      "nota_peso[ 0 ][ 3 ] = nao_assistidos[ 3 ] * matrix[ 0 ][ 3 ] * similarity[ 0 ]\n",
      "nota_peso[ 0 ][ 3 ] =  0  *  4  *  0.9994237971287664\n",
      "nota_peso[ 0 ][ 4 ] = nao_assistidos[ 4 ] * matrix[ 0 ][ 4 ] * similarity[ 0 ]\n",
      "nota_peso[ 0 ][ 4 ] =  1  *  6  *  0.9994237971287664\n",
      "[[0.         0.         0.         0.         5.99654278]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n",
      "nota_peso[ 1 ][ 0 ] = nao_assistidos[ 0 ] * matrix[ 1 ][ 0 ] * similarity[ 1 ]\n",
      "nota_peso[ 1 ][ 0 ] =  0  *  5  *  0.23621543814299703\n",
      "nota_peso[ 1 ][ 1 ] = nao_assistidos[ 1 ] * matrix[ 1 ][ 1 ] * similarity[ 1 ]\n",
      "nota_peso[ 1 ][ 1 ] =  1  *  6  *  0.23621543814299703\n",
      "nota_peso[ 1 ][ 2 ] = nao_assistidos[ 2 ] * matrix[ 1 ][ 2 ] * similarity[ 1 ]\n",
      "nota_peso[ 1 ][ 2 ] =  0  *  1  *  0.23621543814299703\n",
      "nota_peso[ 1 ][ 3 ] = nao_assistidos[ 3 ] * matrix[ 1 ][ 3 ] * similarity[ 1 ]\n",
      "nota_peso[ 1 ][ 3 ] =  0  *  8  *  0.23621543814299703\n",
      "nota_peso[ 1 ][ 4 ] = nao_assistidos[ 4 ] * matrix[ 1 ][ 4 ] * similarity[ 1 ]\n",
      "nota_peso[ 1 ][ 4 ] =  1  *  9  *  0.23621543814299703\n",
      "[[0.         0.         0.         0.         5.99654278]\n",
      " [0.         1.41729263 0.         0.         2.12593894]\n",
      " [0.         0.         0.         0.         0.        ]]\n",
      "nota_peso[ 2 ][ 0 ] = nao_assistidos[ 0 ] * matrix[ 2 ][ 0 ] * similarity[ 2 ]\n",
      "nota_peso[ 2 ][ 0 ] =  0  *  8  *  0.8723686098443353\n",
      "nota_peso[ 2 ][ 1 ] = nao_assistidos[ 1 ] * matrix[ 2 ][ 1 ] * similarity[ 2 ]\n",
      "nota_peso[ 2 ][ 1 ] =  1  *  0  *  0.8723686098443353\n",
      "nota_peso[ 2 ][ 2 ] = nao_assistidos[ 2 ] * matrix[ 2 ][ 2 ] * similarity[ 2 ]\n",
      "nota_peso[ 2 ][ 2 ] =  0  *  0  *  0.8723686098443353\n",
      "nota_peso[ 2 ][ 3 ] = nao_assistidos[ 3 ] * matrix[ 2 ][ 3 ] * similarity[ 2 ]\n",
      "nota_peso[ 2 ][ 3 ] =  0  *  5  *  0.8723686098443353\n",
      "nota_peso[ 2 ][ 4 ] = nao_assistidos[ 4 ] * matrix[ 2 ][ 4 ] * similarity[ 2 ]\n",
      "nota_peso[ 2 ][ 4 ] =  1  *  10  *  0.8723686098443353\n",
      "[[0.         0.         0.         0.         5.99654278]\n",
      " [0.         1.41729263 0.         0.         2.12593894]\n",
      " [0.         0.         0.         0.         8.7236861 ]]\n"
     ]
    }
   ],
   "source": [
    "nota_peso = np.zeros((3, 5))\n",
    "\n",
    "for nUser in range(3):\n",
    "    for nFilme in range(5):\n",
    "        print('nota_peso[',nUser,'][',nFilme,'] = nao_assistidos[', nFilme,'] * matrix[',nUser,'][',nFilme,'] * similarity[',nUser,']')\n",
    "        print('nota_peso[',nUser,'][',nFilme,'] = ', nao_assistidos[nFilme],' * ',matrix[nUser][nFilme], ' * ', similarity[nUser])\n",
    "        nota_peso[nUser][nFilme] = nao_assistidos[nFilme] * matrix[nUser][nFilme] * similarity[nUser]\n",
    "    print(nota_peso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          1.41729263  0.          0.         16.84616782]\n"
     ]
    }
   ],
   "source": [
    "notas_acumuladas = np.sum(nota_peso.T, axis=1)\n",
    "print(notas_acumuladas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "[[0.         0.         0.         0.         0.9994238 ]\n",
      " [0.         0.23621544 0.         0.         0.23621544]\n",
      " [0.         0.         0.         0.         0.87236861]]\n",
      "[0.         0.23621544 0.         0.         2.10800785]\n"
     ]
    }
   ],
   "source": [
    "temp_peso = nota_peso\n",
    "temp_peso[nota_peso > 0] = 1\n",
    "print(temp_peso)\n",
    "\n",
    "temp_similaridade = np.zeros((3, 5))\n",
    "for nUser in range(3):\n",
    "    for nFilme in range(5):\n",
    "        temp_similaridade[nUser][nFilme] = temp_peso[nUser][nFilme] * similarity[nUser]\n",
    "\n",
    "print(temp_similaridade)\n",
    "similaridade_acumulada = np.sum(temp_similaridade.T, axis=1)\n",
    "print(similaridade_acumulada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6.0, 0, 0, 7.991510972567145]\n"
     ]
    }
   ],
   "source": [
    "nota_final = [0] * 5\n",
    "# normalização para cada filme com nota acumulada (pela soma dos pesos)\n",
    "for nFilme in range(5):\n",
    "    if(similaridade_acumulada[nFilme] > 0):\n",
    "        nota_final[nFilme] = notas_acumuladas[nFilme] / similaridade_acumulada[nFilme]\n",
    "    else:\n",
    "        nota_final[nFilme] = 0\n",
    "    \n",
    "print(nota_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1]\n",
      "La Casa de Papel nota:  7.991510972567145\n",
      "A Invocação do Mal nota:  6.0\n"
     ]
    }
   ],
   "source": [
    "nAssistidos = sum(nao_assistidos) # type: ignore\n",
    "\n",
    "notasOrdenadasIndex = sorted(range(len(nota_final)), key=nota_final.__getitem__)[::-1][0:nAssistidos]\n",
    "print(notasOrdenadasIndex)\n",
    "\n",
    "for i in notasOrdenadasIndex:\n",
    "    print(nomeFilmes[i], 'nota: ', nota_final[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row # row formato para o als\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # para verificar a qualidade do modelo, \n",
    "from pyspark.ml.recommendation import ALS # modelo de recomendação (Alternating Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/09 11:42:12 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/09 11:42:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/09 11:42:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text('../../../Data/Fase 3/sample_movielens_ratings.txt').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = lines.map(lambda row: row.value.split('::'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), \\\n",
    "                                    movieId=int(p[1]), \\\n",
    "                                    rating=float(p[2]),  \\\n",
    "                                    timestamp=int(p[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     0|      2|   3.0|1424380312|\n",
      "|     0|      3|   1.0|1424380312|\n",
      "|     0|      5|   2.0|1424380312|\n",
      "|     0|      9|   4.0|1424380312|\n",
      "|     0|     11|   1.0|1424380312|\n",
      "|     0|     12|   2.0|1424380312|\n",
      "|     0|     15|   1.0|1424380312|\n",
      "|     0|     17|   1.0|1424380312|\n",
      "|     0|     19|   1.0|1424380312|\n",
      "|     0|     21|   1.0|1424380312|\n",
      "|     0|     23|   1.0|1424380312|\n",
      "|     0|     26|   3.0|1424380312|\n",
      "|     0|     27|   1.0|1424380312|\n",
      "|     0|     28|   1.0|1424380312|\n",
      "|     0|     29|   1.0|1424380312|\n",
      "|     0|     30|   1.0|1424380312|\n",
      "|     0|     31|   1.0|1424380312|\n",
      "|     0|     34|   1.0|1424380312|\n",
      "|     0|     37|   1.0|1424380312|\n",
      "|     0|     41|   2.0|1424380312|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, \\\n",
    "        regParam=0.01, \\\n",
    "        userCol='userId', \\\n",
    "        itemCol='movieId', \\\n",
    "        ratingCol='rating', \\\n",
    "        coldStartStrategy='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/09 12:02:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/09/09 12:02:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/09/09 12:02:00 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/09/09 12:02:00 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==================================================>  (190 + 10) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro Médio Quadrático: 1.917489553217494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('Erro Médio Quadrático: ' + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRec = model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|    28|[{25, 6.417806}, ...|\n",
      "|    26|[{94, 5.4155145},...|\n",
      "|    27|[{18, 3.9198024},...|\n",
      "|    12|[{17, 5.1947656},...|\n",
      "|    22|[{74, 5.106934}, ...|\n",
      "|     1|[{25, 4.3891563},...|\n",
      "|    13|[{4, 3.232518}, {...|\n",
      "|     6|[{25, 4.615814}, ...|\n",
      "|    16|[{28, 5.4040823},...|\n",
      "|     3|[{32, 5.0642576},...|\n",
      "|    20|[{94, 3.6108603},...|\n",
      "|     5|[{69, 6.417946}, ...|\n",
      "|    19|[{94, 4.0688896},...|\n",
      "|    15|[{90, 5.3539877},...|\n",
      "|    17|[{79, 5.4243345},...|\n",
      "|     9|[{85, 5.5353346},...|\n",
      "|     4|[{83, 4.812314}, ...|\n",
      "|     8|[{53, 5.092467}, ...|\n",
      "|    23|[{7, 6.6129904}, ...|\n",
      "|     7|[{69, 5.243858}, ...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieRec = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     31|[{12, 3.7572966},...|\n",
      "|     85|[{9, 5.5353346}, ...|\n",
      "|     65|[{11, 5.1726727},...|\n",
      "|     53|[{24, 5.590444}, ...|\n",
      "|     78|[{7, 1.6479115}, ...|\n",
      "|     34|[{25, 3.1137831},...|\n",
      "|     81|[{28, 4.544544}, ...|\n",
      "|     28|[{16, 5.4040823},...|\n",
      "|     76|[{14, 4.986357}, ...|\n",
      "|     26|[{15, 2.5443437},...|\n",
      "|     27|[{23, 5.1857514},...|\n",
      "|     44|[{5, 4.0061364}, ...|\n",
      "|     12|[{2, 3.0016155}, ...|\n",
      "|     91|[{11, 4.946367}, ...|\n",
      "|     22|[{26, 5.243876}, ...|\n",
      "|     93|[{2, 5.0935197}, ...|\n",
      "|     47|[{5, 4.1168113}, ...|\n",
      "|      1|[{15, 3.960104}, ...|\n",
      "|     52|[{21, 5.3332176},...|\n",
      "|     13|[{11, 4.047972}, ...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieRec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings.select(als.getUserCol()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|    26|\n",
      "|    29|\n",
      "|    19|\n",
      "|     0|\n",
      "|    22|\n",
      "|     7|\n",
      "|    25|\n",
      "|     6|\n",
      "|     9|\n",
      "|    27|\n",
      "|    17|\n",
      "|    28|\n",
      "|     5|\n",
      "|     1|\n",
      "|    10|\n",
      "|     3|\n",
      "|    12|\n",
      "|     8|\n",
      "|    11|\n",
      "|     2|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserRecsOnlyItemId = userRec.select(userRec['userId'], userRec['recommendations']['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------+\n",
      "|userId|recommendations.movieId                 |\n",
      "+------+----------------------------------------+\n",
      "|28    |[25, 92, 81, 9, 68, 69, 89, 49, 48, 10] |\n",
      "|26    |[94, 51, 22, 23, 24, 68, 75, 28, 74, 46]|\n",
      "|27    |[18, 4, 40, 88, 2, 83, 75, 55, 41, 63]  |\n",
      "|12    |[17, 64, 27, 16, 94, 31, 46, 23, 91, 79]|\n",
      "|22    |[74, 75, 94, 51, 29, 22, 62, 49, 77, 53]|\n",
      "|1     |[25, 49, 69, 62, 68, 75, 24, 51, 67, 22]|\n",
      "|13    |[4, 53, 18, 40, 83, 29, 41, 2, 75, 52]  |\n",
      "|6     |[25, 43, 49, 2, 61, 67, 63, 85, 64, 9]  |\n",
      "|16    |[28, 51, 54, 90, 85, 29, 81, 5, 96, 94] |\n",
      "|3     |[32, 30, 69, 53, 27, 55, 25, 18, 75, 88]|\n",
      "+------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "UserRecsOnlyItemId.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
