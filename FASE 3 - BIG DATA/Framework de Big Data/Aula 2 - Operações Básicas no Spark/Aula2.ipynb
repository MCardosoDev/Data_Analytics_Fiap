{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip istall findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
    "from pyspark.sql import functions as f\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 20:04:00 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/05 20:04:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/05 20:04:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               hello|\n",
      "+--------------------+\n",
      "|Sucesso total, es...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''SELECT 'Sucesso total, estamos online' as hello''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count   : 561\n",
      "df.col ct  : 6\n",
      "df.columns : ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../../Data/Fase 3/banklist.csv', sep=',', inferSchema=True, header=True)\n",
    "\n",
    "print('df.count   :', df.count())\n",
    "print('df.col ct  :', len(df.columns))\n",
    "print('df.columns :', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------------+\n",
      "|Bank Name|City         |Closing Date|\n",
      "+---------+-------------+------------+\n",
      "|Bank Name|Barboursville|Closing Date|\n",
      "|Bank Name|Ericson      |Closing Date|\n",
      "|Bank Name|Newark       |Closing Date|\n",
      "|Bank Name|Maumee       |Closing Date|\n",
      "+---------+-------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('banklist')\n",
    "\n",
    "df_check = spark.sql('''select 'Bank Name', City, 'Closing Date' from banklist''')\n",
    "df_check.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Basics Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "|summary|           Bank Name|   City|  ST|             CERT|Acquiring Institution|Closing Date|\n",
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "|  count|                 561|    561| 561|              561|                  561|         561|\n",
      "|   mean|                null|   null|null|31685.68449197861|                 null|        null|\n",
      "| stddev|                null|   null|null|16446.65659309965|                 null|        null|\n",
      "|    min|1st American Stat...|Acworth|  AL|               91|      1st United Bank|    1-Aug-08|\n",
      "|    max|               ebank|Wyoming|  WY|            58701|  Your Community Bank|    9-Sep-11|\n",
      "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+\n",
      "|summary|   City|  ST|\n",
      "+-------+-------+----+\n",
      "|  count|    561| 561|\n",
      "|   mean|   null|null|\n",
      "| stddev|   null|null|\n",
      "|    min|Acworth|  AL|\n",
      "|    max|Wyoming|  WY|\n",
      "+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('City', 'ST').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count, Columns and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas   : 561\n",
      "Total de colunas   : 6\n",
      "Colunas : ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n",
      "Tipo de dados : [('Bank Name', 'string'), ('City', 'string'), ('ST', 'string'), ('CERT', 'int'), ('Acquiring Institution', 'string'), ('Closing Date', 'string')]\n",
      "Shema : StructType(List(StructField(Bank Name,StringType,true),StructField(City,StringType,true),StructField(ST,StringType,true),StructField(CERT,IntegerType,true),StructField(Acquiring Institution,StringType,true),StructField(Closing Date,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print('Total de linhas   :', df.count())\n",
    "print('Total de colunas   :', len(df.columns))\n",
    "print('Colunas :', df.columns)\n",
    "print('Tipo de dados :', df.dtypes)\n",
    "print('Shema :', df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bank Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ST: string (nullable = true)\n",
      " |-- CERT: integer (nullable = true)\n",
      " |-- Acquiring Institution: string (nullable = true)\n",
      " |-- Closing Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas   : 561\n",
      "Colunas : ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "print('Total de linhas   :', df.count())\n",
    "print('Colunas :', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|           Bank Name|            City|\n",
      "+--------------------+----------------+\n",
      "| First Bank of Idaho|         Ketchum|\n",
      "|Amcore Bank, Nati...|        Rockford|\n",
      "|        Venture Bank|           Lacey|\n",
      "|First State Bank ...|           Altus|\n",
      "|Valley Capital Ba...|            Mesa|\n",
      "|Michigan Heritage...|Farmington Hills|\n",
      "|Columbia Savings ...|      Cincinnati|\n",
      "|       Fidelity Bank|        Dearborn|\n",
      "|The Park Avenue Bank|        Valdosta|\n",
      "|Western Commercia...|  Woodland Hills|\n",
      "|        Syringa Bank|           Boise|\n",
      "|Republic Federal ...|           Miami|\n",
      "|Westside Communit...|University Place|\n",
      "|   First United Bank|           Crete|\n",
      "|HarVest Bank of M...|    Gaithersburg|\n",
      "|            BankEast|       Knoxville|\n",
      "|    Polk County Bank|        Johnston|\n",
      "|Colorado Capital ...|     Castle Rock|\n",
      "|         Access Bank|        Champlin|\n",
      "|Pacific National ...|   San Francisco|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(*['Bank Name', 'City'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+---------------------+--------------------+\n",
      "| CERT|Closing Date|    City|Acquiring Institution|           Bank Name|\n",
      "+-----+------------+--------+---------------------+--------------------+\n",
      "|34396|   24-Apr-09| Ketchum|      U.S. Bank, N.A.| First Bank of Idaho|\n",
      "| 3735|   23-Apr-10|Rockford|          Harris N.A.|Amcore Bank, Nati...|\n",
      "+-----+------------+--------+---------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_1 = list(set(df.columns) - {'CRET', 'ST'})\n",
    "df2 = df.select(*col_1)\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "|           bank_name|            City|state| cert|     acq_institution|closing_date|\n",
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "| First Bank of Idaho|         Ketchum|   ID|34396|     U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|        Rockford|   IL| 3735|         Harris N.A.|   23-Apr-10|\n",
      "|        Venture Bank|           Lacey|   WA|22868|First-Citizens Ba...|   11-Sep-09|\n",
      "|First State Bank ...|           Altus|   OK| 9873|        Herring Bank|   31-Jul-09|\n",
      "|Valley Capital Ba...|            Mesa|   AZ|58399|Enterprise Bank &...|   11-Dec-09|\n",
      "|Michigan Heritage...|Farmington Hills|   MI|34369|      Level One Bank|   24-Apr-09|\n",
      "|Columbia Savings ...|      Cincinnati|   OH|32284|United Fidelity B...|   23-May-14|\n",
      "|       Fidelity Bank|        Dearborn|   MI|33883|The Huntington Na...|   30-Mar-12|\n",
      "|The Park Avenue Bank|        Valdosta|   GA|19797|  Bank of the Ozarks|   29-Apr-11|\n",
      "|Western Commercia...|  Woodland Hills|   CA|58087|First California ...|    5-Nov-10|\n",
      "|        Syringa Bank|           Boise|   ID|34296|        Sunwest Bank|   31-Jan-14|\n",
      "|Republic Federal ...|           Miami|   FL|22846|     1st United Bank|   11-Dec-09|\n",
      "|Westside Communit...|University Place|   WA|33997|        Sunwest Bank|   11-Jan-13|\n",
      "|   First United Bank|           Crete|   IL|20685|Old Plank Trail C...|   28-Sep-12|\n",
      "|HarVest Bank of M...|    Gaithersburg|   MD|57766|            Sonabank|   27-Apr-12|\n",
      "|            BankEast|       Knoxville|   TN|19869|     U.S. Bank, N.A.|   27-Jan-12|\n",
      "|    Polk County Bank|        Johnston|   IA|14194| Grinnell State Bank|   18-Nov-11|\n",
      "|Colorado Capital ...|     Castle Rock|   CO|34522|First-Citizens Ba...|    8-Jul-11|\n",
      "|         Access Bank|        Champlin|   MN|16476|           PrinsBank|    7-May-10|\n",
      "|Pacific National ...|   San Francisco|   CA|30006|      U.S. Bank N.A.|   30-Oct-09|\n",
      "+--------------------+----------------+-----+-----+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df \\\n",
    "    .withColumnRenamed('Bank Name', 'bank_name') \\\n",
    "    .withColumnRenamed('Acquiring Institution', 'acq_institution') \\\n",
    "    .withColumnRenamed('Closing Date', 'closing_date') \\\n",
    "    .withColumnRenamed('ST', 'state') \\\n",
    "    .withColumnRenamed('CERT', 'cert')\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|state|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('state', col('ST'))\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add constant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+-----+-------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|state|country|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+-------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|     US|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|     US|\n",
      "+--------------------+--------+---+-----+---------------------+------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('country', lit('US'))\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+---------------------+------------+\n",
      "|           Bank Name|    City| ST|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop('CERT')\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Multiple Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------------+------------+\n",
      "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop(*['ST', 'CERT'])\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------------+------------+\n",
      "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = reduce(DataFrame.drop, ['CERT', 'ST'], df)\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count:  561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2.count:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3.count:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=============================================>        (169 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df4.count:  73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Equal to values\n",
    "df2 = df.where(df['ST'] == 'NE')\n",
    "\n",
    "# Between values\n",
    "df3 = df.where(df['CERT'].between('1000', '2000'))\n",
    "\n",
    "# Is inside multiple values\n",
    "df4 = df.where(df['ST'].isin('NE', 'IL'))\n",
    "\n",
    "print('df.count: ', df.count())\n",
    "print('df2.count: ', df2.count())\n",
    "print('df3.count: ', df3.count())\n",
    "print('df4.count: ', df4.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data using Logical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "|         Bank Name|   City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "|Ericson State Bank|Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
      "+------------------+-------+---+-----+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.where((df['ST'] == 'NE') & (df['City'] == 'Ericson'))\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Values in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Replace 7 in the above dataframe with 17 at all instances\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
      "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
      "+--------------------+--------+---+-----+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre replace\n",
    "df.show(2)\n",
    "\n",
    "# Post replace\n",
    "print('Replace 7 in the above dataframe with 17 at all instances')\n",
    "df.na.replace(7,17).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 21:47:33 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/05 21:47:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/05 21:47:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySpark Dataframe From RDD').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PySpark DataFrame from an Existing RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('C', 85, 76, 87, 91), ('B', 85, 76, 87, 91), ('A', 85, 78, 96, 92), ('A', 92, 76, 89, 96)], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = ['id_person', 'value_1', 'value_2', 'value_3', 'value_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "marks_df = spark.createDataFrame(rdd, schema=sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(marks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_person: string (nullable = true)\n",
      " |-- value_1: long (nullable = true)\n",
      " |-- value_2: long (nullable = true)\n",
      " |-- value_3: long (nullable = true)\n",
      " |-- value_4: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+-------+\n",
      "|id_person|value_1|value_2|value_3|value_4|\n",
      "+---------+-------+-------+-------+-------+\n",
      "|        C|     85|     76|     87|     91|\n",
      "|        B|     85|     76|     87|     91|\n",
      "|        A|     85|     78|     96|     92|\n",
      "|        A|     92|     76|     89|     96|\n",
      "+---------+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Manipulation in PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 11:09:24 WARN Utils: Your hostname, MacBook-Air-de-Marcelo.local resolves to a loopback address: 127.0.0.1; using 192.168.0.104 instead (on interface en0)\n",
      "23/09/06 11:09:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/06 11:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pysparkdf').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../../Data/Fase 3/cereal.csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---------+\n",
      "|                name|mfr|   rating|\n",
      "+--------------------+---+---------+\n",
      "|           100% Bran|  N|68.402973|\n",
      "|   100% Natural Bran|  Q|33.983679|\n",
      "|            All-Bran|  K|59.425505|\n",
      "|All-Bran with Ext...|  K|93.704912|\n",
      "|      Almond Delight|  R|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|29.509541|\n",
      "|         Apple Jacks|  K|33.174094|\n",
      "|             Basic 4|  G|37.038562|\n",
      "|           Bran Chex|  R|49.120253|\n",
      "|         Bran Flakes|  P|53.313813|\n",
      "|        Cap'n'Crunch|  Q|18.042851|\n",
      "|            Cheerios|  G|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|19.823573|\n",
      "|            Clusters|  G|40.400208|\n",
      "|         Cocoa Puffs|  G|22.736446|\n",
      "|           Corn Chex|  R|41.445019|\n",
      "|         Corn Flakes|  K|45.863324|\n",
      "|           Corn Pops|  K|35.782791|\n",
      "|       Count Chocula|  G|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|40.448772|\n",
      "+--------------------+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'mfr', 'rating').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- Calories: integer (nullable = true)\n",
      " |-- protein: integer (nullable = true)\n",
      " |-- fat: integer (nullable = true)\n",
      " |-- sodium: integer (nullable = true)\n",
      " |-- fiber: double (nullable = true)\n",
      " |-- carbo: double (nullable = true)\n",
      " |-- sugars: integer (nullable = true)\n",
      " |-- potass: integer (nullable = true)\n",
      " |-- vitamins: integer (nullable = true)\n",
      " |-- shelf: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- cups: double (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Calories', df['calories'].cast('Integer')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Calories|count|\n",
      "+--------+-----+\n",
      "|     140|    3|\n",
      "|     120|   10|\n",
      "|     100|   17|\n",
      "|     130|    2|\n",
      "|      50|    3|\n",
      "|      80|    1|\n",
      "|     160|    1|\n",
      "|      70|    2|\n",
      "|      90|    7|\n",
      "|     110|   29|\n",
      "|     150|    2|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('Calories').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|        Puffed Wheat|  Q|   C|      50|      2|  0|     0|  1.0| 10.0|     0|    50|       0|    3|   0.5| 1.0|63.005645|\n",
      "|         Puffed Rice|  Q|   C|      50|      1|  0|     0|  0.0| 13.0|     0|    15|       0|    3|   0.5| 1.0|60.756112|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|      Shredded Wheat|  N|   C|      80|      2|  0|     0|  3.0| 16.0|     0|    95|       0|    1|  0.83| 1.0|68.235885|\n",
      "|Strawberry Fruit ...|  N|   C|      90|      2|  0|    15|  3.0| 15.0|     5|    90|      25|    2|   1.0| 1.0|59.363993|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|Shredded Wheat 'n...|  N|   C|      90|      3|  0|     0|  4.0| 19.0|     0|   140|       0|    1|   1.0|0.67|74.472949|\n",
      "|Shredded Wheat sp...|  N|   C|      90|      3|  0|     0|  3.0| 20.0|     0|   120|       0|    1|   1.0|0.67|72.801787|\n",
      "|   Nutri-grain Wheat|  K|   C|      90|      3|  0|   170|  3.0| 18.0|     2|    90|      25|    3|   1.0| 1.0|59.642837|\n",
      "|      Raisin Squares|  K|   C|      90|      2|  0|     0|  2.0| 15.0|     6|   110|      25|    3|   1.0| 0.5|55.333142|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "|   Grape Nuts Flakes|  P|   C|     100|      3|  1|   140|  3.0| 15.0|     5|    85|      25|    3|   1.0|0.88|52.076897|\n",
      "| Frosted Mini-Wheats|  K|   C|     100|      3|  0|     0|  3.0| 14.0|     7|   100|      25|    2|   1.0| 0.8|58.345141|\n",
      "|                Life|  Q|   C|     100|      4|  2|   150|  2.0| 12.0|     6|    95|      25|    2|   1.0|0.67|45.328074|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('calories').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------------------------------------------------+\n",
      "|                name|vitamins|CASE WHEN (vitamins >= 25) THEN rich in vitamins END|\n",
      "+--------------------+--------+----------------------------------------------------+\n",
      "|           100% Bran|      25|                                    rich in vitamins|\n",
      "|   100% Natural Bran|       0|                                                null|\n",
      "|            All-Bran|      25|                                    rich in vitamins|\n",
      "|All-Bran with Ext...|      25|                                    rich in vitamins|\n",
      "|      Almond Delight|      25|                                    rich in vitamins|\n",
      "|Apple Cinnamon Ch...|      25|                                    rich in vitamins|\n",
      "|         Apple Jacks|      25|                                    rich in vitamins|\n",
      "|             Basic 4|      25|                                    rich in vitamins|\n",
      "|           Bran Chex|      25|                                    rich in vitamins|\n",
      "|         Bran Flakes|      25|                                    rich in vitamins|\n",
      "|        Cap'n'Crunch|      25|                                    rich in vitamins|\n",
      "|            Cheerios|      25|                                    rich in vitamins|\n",
      "|Cinnamon Toast Cr...|      25|                                    rich in vitamins|\n",
      "|            Clusters|      25|                                    rich in vitamins|\n",
      "|         Cocoa Puffs|      25|                                    rich in vitamins|\n",
      "|           Corn Chex|      25|                                    rich in vitamins|\n",
      "|         Corn Flakes|      25|                                    rich in vitamins|\n",
      "|           Corn Pops|      25|                                    rich in vitamins|\n",
      "|       Count Chocula|      25|                                    rich in vitamins|\n",
      "|  Cracklin' Oat Bran|      25|                                    rich in vitamins|\n",
      "+--------------------+--------+----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'vitamins', when(df.vitamins >= '25', 'rich in vitamins')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "|Cream of Wheat (Q...|  N|   H|     100|      3|  0|    80|  1.0| 21.0|     0|    -1|       0|    2|   1.0| 1.0|64.533816|\n",
      "|             Crispix|  K|   C|     110|      2|  0|   220|  1.0| 21.0|     3|    30|      25|    3|   1.0| 1.0|46.895644|\n",
      "|Crispy Wheat & Ra...|  G|   C|     100|      2|  1|   140|  2.0| 11.0|    10|   120|      25|    3|   1.0|0.75|36.176196|\n",
      "|         Double Chex|  R|   C|     100|      2|  0|   190|  1.0| 18.0|     5|    80|      25|    3|   1.0|0.75|44.330856|\n",
      "|         Froot Loops|  K|   C|     110|      2|  1|   125|  1.0| 11.0|    13|    30|      25|    2|   1.0| 1.0|32.207582|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.calories >= '100').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isnull() / isnotnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|   1.0|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|  1.0| 11.0|    14|    30|      25|    2|   1.0| 1.0|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|  2.0| 18.0|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|  4.0| 15.0|     6|   125|      25|    1|   1.0|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|  5.0| 13.0|     5|   190|      25|    3|   1.0|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|  0.0| 12.0|    12|    35|      25|    2|   1.0|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|  2.0| 17.0|     1|   105|      25|    1|   1.0|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|  0.0| 13.0|     9|    45|      25|    2|   1.0|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|  2.0| 13.0|     7|   105|      25|    3|   1.0| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    55|      25|    2|   1.0| 1.0|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|  0.0| 22.0|     3|    25|      25|    1|   1.0| 1.0|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|  1.0| 21.0|     2|    35|      25|    1|   1.0| 1.0|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|  1.0| 13.0|    12|    20|      25|    2|   1.0| 1.0|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|  0.0| 12.0|    13|    65|      25|    2|   1.0| 1.0|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|  4.0| 10.0|     7|   160|      25|    3|   1.0| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "|name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|rating|\n",
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "+----+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
